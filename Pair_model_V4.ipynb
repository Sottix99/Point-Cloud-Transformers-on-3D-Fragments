{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import special_ortho_group\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning import miners, losses\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=999\n",
    "os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msottile124\u001b[0m (\u001b[33mpair_fragments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_GPU():\n",
    "    \"\"\" This function activates the gpu \n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(torch.cuda.get_device_name(0), \"is available and being used\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available, using CPU instead\") \n",
    "    return device  \n",
    "\n",
    "\n",
    "def center_in_origin(frag):\n",
    "    \"\"\" This function normalize each fragment in (0,1)\n",
    "    \"\"\"\n",
    "    min_vals, _ = torch.min(frag[:, 0:3], axis=0)\n",
    "    max_vals, _ = torch.max(frag[:, 0:3], axis=0)\n",
    "    frag[:, 0:3] = (frag[:, 0:3] - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    return frag\n",
    "    \n",
    "def normalize(batch):\n",
    "    \"\"\" This function apply center_in_origin() to each fragment in the batch\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for element in batch:\n",
    "        out.append(center_in_origin(element))\n",
    "    out_tensor = torch.stack(out)    \n",
    "    return out_tensor  \n",
    "\n",
    "\n",
    "\n",
    "def translate_to_origin(frag):\n",
    "    \"\"\" This function translate each fragment in the origin\n",
    "    \"\"\"\n",
    "    frag[:,:3] -= torch.mean(frag[:,:3]) \n",
    "    return frag\n",
    "\n",
    "def apply_translation(batch):\n",
    "    \"\"\" This function apply translate_to_origin() to each fragment in the batch\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for element in batch:\n",
    "        out.append(translate_to_origin(element))\n",
    "    out_tensor = torch.stack(out)    \n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "def random_rotation(frag):\n",
    "\n",
    "    randrot = (torch.rand(3)*360).tolist()\n",
    "    r = R.from_euler('zyx', randrot, degrees=True)\n",
    "    frag[:,:3] = torch.from_numpy(r.apply(frag[:,:3]))\n",
    "    frag[:,3:6] = torch.from_numpy(r.apply(frag[:,3:6]))\n",
    "    return frag\n",
    "\n",
    "def apply_randomrotations(batch):\n",
    "    \"\"\" This function apply random_rotation() to each fragment in the batch\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for element in batch:\n",
    "        out.append(random_rotation(element))\n",
    "    out_tensor = torch.stack(out)    \n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code form the repository of PCT https://github.com/qq456cvb/Point-Transformers\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    return torch.sum((src[:, :, None] - dst[:, None]) ** 2, dim=-1)\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S, [K]]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, [K], C]\n",
    "    \"\"\"\n",
    "    raw_size = idx.size()\n",
    "    idx = idx.reshape(raw_size[0], -1)\n",
    "    res = torch.gather(points, 1, idx[..., None].expand(-1, -1, points.size(-1)))\n",
    "    return res.reshape(*raw_size, -1)\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        distance = torch.min(distance, dist)\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def sample_and_group(npoint, nsample, xyz, points):\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint \n",
    "    \n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint]\n",
    "\n",
    "    new_xyz = index_points(xyz, fps_idx) \n",
    "    new_points = index_points(points, fps_idx)\n",
    "\n",
    "    dists = square_distance(new_xyz, xyz)  # B x npoint x N\n",
    "    idx = dists.argsort()[:, :, :nsample]  # B x npoint x K\n",
    "\n",
    "    grouped_points = index_points(points, idx)\n",
    "    grouped_points_norm = grouped_points - new_points.view(B, S, 1, -1)\n",
    "    new_points = torch.cat([grouped_points_norm, new_points.view(B, S, 1, -1).repeat(1, 1, nsample, 1)], dim=-1)\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class Local_op(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, s, d = x.size()  # torch.Size([32, 512, 32, 6]) \n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.reshape(-1, d, s)\n",
    "        batch_size, _, N = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x))) # B, D, N\n",
    "        x = torch.max(x, 2)[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = x.reshape(b, n, -1).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SA_Layer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(channels, channels // 4, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(channels, channels // 4, 1, bias=False)\n",
    "        self.q_conv.weight = self.k_conv.weight \n",
    "        self.v_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.trans_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_q = self.q_conv(x).permute(0, 2, 1) # b, n, c \n",
    "        x_k = self.k_conv(x)# b, c, n        \n",
    "        x_v = self.v_conv(x)\n",
    "        energy = x_q @ x_k # b, n, n \n",
    "        attention = self.softmax(energy)\n",
    "        attention = attention / (1e-9 + attention.sum(dim=1, keepdims=True))\n",
    "        x_r = x_v @ attention # b, c, n \n",
    "        x_r = self.act(self.after_norm(self.trans_conv(x - x_r)))\n",
    "        x = x + x_r\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class StackedAttention(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "\n",
    "        self.sa1 = SA_Layer(channels)\n",
    "        self.sa2 = SA_Layer(channels)\n",
    "        self.sa3 = SA_Layer(channels)\n",
    "        self.sa4 = SA_Layer(channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # \n",
    "        # b, 3, npoint, nsample  \n",
    "        # conv2d 3 -> 128 channels 1, 1\n",
    "        # b * npoint, c, nsample \n",
    "        # permute reshape\n",
    "        batch_size, _, N = x.size()\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x1 = self.sa1(x)\n",
    "        x2 = self.sa2(x1)\n",
    "        x3 = self.sa3(x2)\n",
    "        x4 = self.sa4(x3)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same as before but witch classical attention \n",
    "class SA_Layer_classic(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(channels, channels // 4, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(channels, channels // 4, 1, bias=False)\n",
    "        self.q_conv.weight = self.k_conv.weight \n",
    "        self.v_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.trans_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_q = self.q_conv(x).permute(0, 2, 1) # b, n, c \n",
    "        x_k = self.k_conv(x)# b, c, n        \n",
    "        x_v = self.v_conv(x)\n",
    "        energy = x_q @ x_k # b, n, n \n",
    "        attention = self.softmax(energy)\n",
    "        attention = attention / (1e-9 + attention.sum(dim=1, keepdims=True))\n",
    "        x_r = x_v @ attention # b, c, n \n",
    "        x_r = self.act(self.after_norm(self.trans_conv(x_r)))\n",
    "        \n",
    "        return x_r\n",
    "\n",
    "\n",
    "class StackedAttention_classic(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "\n",
    "        self.sa1 = SA_Layer_classic(channels)\n",
    "        self.sa2 = SA_Layer_classic(channels)\n",
    "        self.sa3 = SA_Layer_classic(channels)\n",
    "        self.sa4 = SA_Layer_classic(channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # \n",
    "        # b, 3, npoint, nsample  \n",
    "        # conv2d 3 -> 128 channels 1, 1\n",
    "        # b * npoint, c, nsample \n",
    "        # permute reshape\n",
    "        batch_size, _, N = x.size()\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x1 = self.sa1(x)\n",
    "        x2 = self.sa2(x1)\n",
    "        x3 = self.sa3(x2)\n",
    "        x4 = self.sa4(x3)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Branch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_points = 7 # we have 7 features for each point\n",
    "        self.conv1 = nn.Conv1d(d_points, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.gather_local_0 = Local_op(in_channels=128, out_channels=128)\n",
    "        self.gather_local_1 = Local_op(in_channels=256, out_channels=256)\n",
    "        self.pt_last = StackedAttention()###############################################################\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv_fuse = nn.Sequential(nn.Conv1d(1280, 1024, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm1d(1024),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xyz = x[..., :3]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        batch_size, _, _ = x.size()\n",
    "        x= x.double()\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x))) # B, D, N\n",
    "        x = x.permute(0, 2, 1)\n",
    "        new_xyz, new_feature = sample_and_group(npoint=512, nsample=32, xyz=xyz, points=x)         \n",
    "        feature_0 = self.gather_local_0(new_feature)\n",
    "        feature = feature_0.permute(0, 2, 1)\n",
    "        new_xyz, new_feature = sample_and_group(npoint=256, nsample=32, xyz=new_xyz, points=feature) \n",
    "        feature_1 = self.gather_local_1(new_feature)\n",
    "        \n",
    "        x = self.pt_last(feature_1)\n",
    "        x = torch.cat([x, feature_1], dim=1)\n",
    "        x = self.conv_fuse(x)\n",
    "        x = torch.max(x, 2)[0] # Returns the maximum value of all elements in the input tensor. (2 elementes for each vector)\n",
    "        x = x.view(batch_size, -1) # Returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "def apply_cosine_sim(A,B):\n",
    "    cosine_similarities = F.cosine_similarity(A, B)\n",
    "\n",
    "    min_value = torch.min(cosine_similarities)\n",
    "    max_value = torch.max(cosine_similarities)\n",
    "    scaled_similarities = (cosine_similarities - min_value) / (max_value - min_value)\n",
    "    one_minus_scaled = 1 - scaled_similarities\n",
    "    result_tensor = torch.cat((one_minus_scaled.view(-1, 1), scaled_similarities.view(-1, 1)), dim=1) \n",
    "    return result_tensor    \n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, m=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, y1, y2, d):\n",
    "        euc_dist = torch.nn.functional.pairwise_distance(y1, y2)\n",
    "\n",
    "        if d.dim() == 0:  # Se d è uno scalare\n",
    "            if d == 0:\n",
    "                return torch.mean(torch.pow(euc_dist, 2))  # Distanza quadratica\n",
    "            else:  # d == 1\n",
    "                delta = self.m - euc_dist\n",
    "                delta = torch.clamp(delta, min=0.0, max=None)\n",
    "                return torch.mean(torch.pow(delta, 2))\n",
    "        else:  # Se d è un tensore di valori 0 e 1\n",
    "            is_same = d == 0\n",
    "            is_diff = d == 1\n",
    "\n",
    "            loss_same = torch.pow(euc_dist[is_same], 2).mean() if torch.any(is_same) else torch.tensor(0.0).to(euc_dist.device)\n",
    "            loss_diff = torch.pow(torch.clamp(self.m - euc_dist[is_diff], min=0.0), 2).mean() if torch.any(is_diff) else torch.tensor(0.0).to(euc_dist.device)\n",
    "\n",
    "            return (loss_same + loss_diff) / (1.0 + torch.any(is_same).float() + torch.any(is_diff).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "train = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\train_pair_dataset_REG.pt\")\n",
    "val = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\val_pair_dataset_REG.pt\")\n",
    "test = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\test_pair_dataset_REG.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions to remove (Train): [2, 14, 20, 22, 28, 30, 31, 35, 36, 39, 71, 78, 87, 91, 95, 109, 136, 148, 163, 167, 192, 197, 205, 209, 215, 217, 218, 222, 228, 241, 255, 261, 263, 265, 273, 284, 300, 308, 318, 337, 344, 346, 353, 361, 364, 369, 398, 420, 423, 427, 431, 433, 468, 469, 471, 474, 489, 494, 496, 507, 513, 516, 517, 538, 547, 551, 564, 577, 591, 595, 597, 600, 607, 613, 617, 633, 635, 644, 651, 652, 660, 664, 675, 689, 694, 699, 700, 707, 711, 721, 731, 735, 754, 757, 758, 763, 768, 771, 776, 783, 789, 813, 818, 831, 834, 837, 841, 844, 846, 874, 879, 896, 909, 917, 931, 943, 971, 977, 979, 988, 991, 994, 996, 999, 1042, 1045, 1050, 1057, 1077, 1078, 1079, 1084, 1100, 1109, 1118, 1126, 1127, 1129, 1130, 1131, 1143, 1150, 1156, 1188, 1217, 1219, 1227, 1254, 1274, 1284, 1290, 1295, 1299, 1316, 1324, 1334, 1336, 1337, 1347, 1353, 1356, 1390, 1392, 1412, 1418, 1460, 1463, 1466, 1469, 1474, 1483, 1489, 1498, 1499, 1504, 1505, 1507, 1514]\n",
      "Positions to remove (Val): [6, 12, 13, 22, 24, 31, 65, 70, 77, 94, 105, 118, 125, 137, 145, 147, 157, 162, 169, 173, 177, 180, 187, 192, 196, 216, 219, 241, 255, 281, 293, 294, 300, 301, 308]\n",
      "Positions to remove (Test): [3, 34, 36, 41, 48, 50, 60, 69, 91, 92, 93, 96, 99, 112, 123, 135, 140, 141, 151, 166, 177, 186, 189, 232, 238, 244, 246, 251, 255, 262, 266, 267, 271, 304, 312, 316, 322, 326]\n",
      "(1348, 2)\n",
      "(292, 2)\n",
      "(289, 2)\n"
     ]
    }
   ],
   "source": [
    "#let's find the largest clusters\n",
    "\n",
    "threshold = 70\n",
    "\n",
    "# Train\n",
    "indices = [i for i, data in enumerate(train) if data[0].shape[0] > threshold]\n",
    "count = len(indices)\n",
    "\n",
    "# Val\n",
    "indices_val = [i for i, data in enumerate(val) if data[0].shape[0] > threshold]\n",
    "count_val = len(indices_val)\n",
    "\n",
    "# Test\n",
    "indices_test = [i for i, data in enumerate(test) if data[0].shape[0] > threshold]\n",
    "count_test = len(indices_test)\n",
    "\n",
    "print(\"Positions to remove (Train):\", indices)\n",
    "print(\"Positions to remove (Val):\", indices_val)\n",
    "print(\"Positions to remove (Test):\", indices_test)\n",
    "\n",
    "\n",
    "# We are removing the largest clusters for computational reasons\n",
    "mask = torch.ones(train.shape[0], dtype=torch.bool)\n",
    "mask[indices] = False\n",
    "filtered_tensor = train[mask]\n",
    "train = filtered_tensor\n",
    "\n",
    "mask_val = torch.ones(val.shape[0], dtype=torch.bool)\n",
    "mask_val[indices_val] = False\n",
    "filtered_tensor_val = val[mask_val]\n",
    "val = filtered_tensor_val\n",
    "\n",
    "mask_test = torch.ones(test.shape[0], dtype=torch.bool)\n",
    "mask_test[indices_test] = False\n",
    "filtered_tensor_test = test[mask_test]\n",
    "test= filtered_tensor_test   \n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pos (vector):\n",
    "\n",
    "    positions = np.arange(len(vector))  \n",
    "    \n",
    "    return positions, vector\n",
    "\n",
    "\n",
    "def fixed_and_balanced (vector):\n",
    "\n",
    "    labels = torch.Tensor(vector)\n",
    "    num = min((labels==0).int().sum(), (labels==1).int().sum())\n",
    "    \n",
    "    zero_indices = (labels == 0).nonzero()[:num, 0]\n",
    "    one_indices = (labels == 1).nonzero()[:num, 0]\n",
    "    combined_indices = torch.sort(torch.cat((zero_indices, one_indices)))\n",
    "    \n",
    "    selected_labels = labels[combined_indices.values]\n",
    "    return combined_indices.values.tolist(), selected_labels \n",
    "\n",
    "def fixed_and_unbalanced (vector):\n",
    "     \n",
    "    labels = torch.Tensor(vector)\n",
    "    num_zeroes = (labels==0).int().sum() \n",
    "    num_ones = (labels==1).int().sum()\n",
    "    num = min(num_zeroes, num_ones)\n",
    "    \n",
    "    extra_1= int((num_ones-num_zeroes)/3)\n",
    "    extra_0= int((num_zeroes-num_ones)/3)\n",
    "    \n",
    "    zero_indices = (labels == 0).nonzero()[:num + extra_0, 0]\n",
    "    one_indices = (labels == 1).nonzero()[:num + extra_1, 0]\n",
    "    combined_indices = torch.sort(torch.cat((zero_indices, one_indices)))\n",
    "    \n",
    "    selected_labels = labels[combined_indices.values]\n",
    "    return combined_indices.values.tolist(), selected_labels  \n",
    "\n",
    "def sampled_balanced (vector):\n",
    "\n",
    "    labels = torch.Tensor(vector)\n",
    "    num_zeroes = (labels==0).int().sum() \n",
    "    num_ones = (labels==1).int().sum()\n",
    "    num = min(num_zeroes, num_ones)\n",
    "\n",
    "    zero_indices = (labels == 0).nonzero()[:num, 0]\n",
    "    one_indices = (labels == 1).nonzero()[:num, 0]\n",
    "\n",
    "    one_indices = one_indices[torch.randperm(one_indices.size(0))][:num]\n",
    "    zero_indices = zero_indices[torch.randperm(zero_indices.size(0))][:num]\n",
    "     \n",
    "    combined_indices = torch.sort(torch.cat((zero_indices, one_indices)))\n",
    "    \n",
    "    selected_labels = labels[combined_indices.values]\n",
    "    \n",
    "    return combined_indices.values.tolist(), selected_labels \n",
    "\n",
    "\n",
    "    \n",
    "def select_couples(y,type_):\n",
    "    \"\"\" \n",
    "    ## type == 0 : select all the positions\n",
    "\n",
    "    ## type == 1: let k = min(numbers of zeros, numbers of ones), select the first k zeros and the first k ones every time (always the same couples in each cluster)\n",
    "           \n",
    "    ## type == 2: let k = min(numbers of zeros, numbers of ones), select all the k elements of the minority class and  k + (#majority - #minority)/3 (always the same couples in each cluster)\n",
    "        \n",
    "    ## type == 3 : let k = min(numbers of zeros, numbers of ones), select all the k elements of the minority class and sample k elements from the majority one\"\"\"\n",
    "\n",
    "\n",
    "    if type_ == 0:\n",
    "                # all the position (unbalanced)\n",
    "            positions, lab = all_pos(y)\n",
    "    elif type_ == 1:\n",
    "            # same 0 and 1 (balanced)\n",
    "            positions, lab = fixed_and_balanced(y)\n",
    "            \n",
    "    elif type_ == 2:\n",
    "            # not the same 0 and 1 (unbalanced)\n",
    "            positions, lab = fixed_and_unbalanced(y)\n",
    "    elif type_ == 3: \n",
    "            # same 0 and 1 + sampling (balanced)    \n",
    "            positions, lab = sampled_balanced(y)\n",
    "    else:\n",
    "            raise ValueError(\"Type can be only  0, 1, 2 or 3\")\n",
    "    return positions, lab    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        output_channels = 2 # it's a binary classification\n",
    "\n",
    "      \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "        # classificator\n",
    "        self.linear0 = nn.Linear(2048, 1024, bias=False)\n",
    "        self.bn0 = nn.BatchNorm1d(1024)\n",
    "        self.dp0 = nn.Dropout(p=0.5)\n",
    "        self.linear1 = nn.Linear(1024, 512, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=0.3)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dp3 = nn.Dropout(p=0.3)\n",
    "        self.linear4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dp4 = nn.Dropout(p=0.3)\n",
    "        self.linear5 = nn.Linear(64, output_channels)\n",
    "        \n",
    "    def forward(self, x_1, x_2):\n",
    "       \n",
    "        x_mult = x_1 * x_2 # sum the two elements of the couples\n",
    "        x_sum = x_1 * x_2 # multiply the two elements of the couples\n",
    "        x = torch.cat((x_mult, x_sum), dim = 1) \n",
    "\n",
    "        # classificator\n",
    "        if x_1.shape[0] > 1:\n",
    "            x = self.relu(self.bn0(self.linear0(x)))\n",
    "            #x = self.dp0(x)\n",
    "            x = self.relu(self.bn1(self.linear1(x)))\n",
    "            x = self.dp1(x)\n",
    "            x = self.relu(self.bn2(self.linear2(x)))\n",
    "            x = self.dp2(x)\n",
    "            x = self.relu(self.bn3(self.linear3(x)))\n",
    "            x = self.dp3(x)\n",
    "            x = self.relu(self.bn4(self.linear4(x)))\n",
    "            x = self.dp4(x)\n",
    "            x = self.linear5(x)\n",
    "\n",
    "        # If we have only one pair, applying batch normalization doesn't make sense.\n",
    "        else: \n",
    "            x = self.relu(self.linear0(x))\n",
    "            #x = self.dp0(x)\n",
    "            x = self.relu(self.linear1(x))\n",
    "            x = self.dp1(x)\n",
    "            x = self.relu(self.linear2(x))\n",
    "            x = self.dp2(x)\n",
    "            x = self.relu(self.linear3(x))\n",
    "            x = self.dp3(x)\n",
    "            x = self.relu(self.linear4(x))\n",
    "            x = self.dp4(x)\n",
    "            x = self.linear5(x)\n",
    "                    \n",
    "        return x\n",
    "\n",
    "\n",
    "class Model2_mod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptc_net = Branch()\n",
    "        self.pair_net = PairModel2()      \n",
    "\n",
    "    def forward(self,cluster, positions, n_frags):\n",
    "        \n",
    "        cluster = cluster.double().to(device)\n",
    "\n",
    "        # Salva l'ordine originale dei dati\n",
    "        original_order = np.arange(len(cluster))\n",
    "        # Esegui lo shuffle dei dati\n",
    "        np.random.shuffle(original_order)\n",
    "        cluster = cluster[original_order]\n",
    "\n",
    "        # create an empty tensor that will collect the trasformed fragments\n",
    "        cluster_transformed = torch.Tensor([]).to(device)\n",
    "\n",
    "        sub_cluster_start = 0\n",
    "        while sub_cluster_start < n_frags:\n",
    "            sub_cluster_end = min(sub_cluster_start + 16, n_frags)\n",
    "            cluster_subset = cluster[sub_cluster_start:sub_cluster_end]\n",
    "            cluster_subset = cluster_subset.to(device)\n",
    "            # apply the PCT\n",
    "            point_clouds_transformed = self.ptc_net(cluster_subset)\n",
    "            # append the results\n",
    "            cluster_transformed = torch.cat((cluster_transformed, point_clouds_transformed), dim=0)\n",
    "            sub_cluster_start += 16\n",
    "\n",
    "        sorted_indices = np.argsort(original_order)\n",
    "        cluster_transformed = cluster_transformed[sorted_indices]\n",
    "        \n",
    "        # given the positions and the transformed cluster create the couples\n",
    "        indices = torch.triu_indices(n_frags, n_frags, offset=1)\n",
    "    \n",
    "        # Seleziona solo gli indici che corrispondono a posizioni desiderate\n",
    "        selected_indices = indices[:, positions]\n",
    "        \n",
    "        # Estrai i tensori dai frammenti selezionati\n",
    "        frags_a = cluster_transformed[selected_indices[0]]\n",
    "        frags_b = cluster_transformed[selected_indices[1]] \n",
    "\n",
    "        frags_a = frags_a.double().to(device)\n",
    "        frags_b = frags_b.double().to(device)\n",
    "    \n",
    "        outputs = self.pair_net(frags_a, frags_b)  \n",
    "        return outputs, frags_a, frags_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4080 is available and being used\n"
     ]
    }
   ],
   "source": [
    "device=use_GPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.tolist()\n",
    "val = val.tolist()\n",
    "# We process one cluster at a time.\n",
    "train_loader = DataLoader(train, batch_size=1)\n",
    "val_loader = DataLoader(val, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2_mod(\n",
       "  (ptc_net): Branch(\n",
       "    (conv1): Conv1d(7, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (gather_local_0): Local_op(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (gather_local_1): Local_op(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pt_last): StackedAttention(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sa1): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa2): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa3): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa4): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (conv_fuse): Sequential(\n",
       "      (0): Conv1d(1280, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (pair_net): PairModel2(\n",
       "    (relu): ReLU()\n",
       "    (linear0): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "    (bn0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp0): Dropout(p=0.5, inplace=False)\n",
       "    (linear1): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp1): Dropout(p=0.2, inplace=False)\n",
       "    (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp2): Dropout(p=0.3, inplace=False)\n",
       "    (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp3): Dropout(p=0.3, inplace=False)\n",
       "    (linear4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp4): Dropout(p=0.3, inplace=False)\n",
       "    (linear5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model2_mod().to(device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_stored = torch.load(r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\Check_points\\\\18_10_1616_1_vff.pt')  # run Not_sampled_0.0003 on w and B site\n",
    "\n",
    "model.load_state_dict(W_stored)\n",
    "#1012_164220_4+lr=0.0003+balanced_coupels.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:an5zhbvd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dc8b1af7da453d8fc3ef3226972dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-sun-7</strong> at: <a href='https://wandb.ai/pair_fragments/Tentativo_nuovo/runs/an5zhbvd' target=\"_blank\">https://wandb.ai/pair_fragments/Tentativo_nuovo/runs/an5zhbvd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231018_155026-an5zhbvd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:an5zhbvd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35c4036e09c491b919e3c0d99cdea57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Alessandro\\Desktop\\Tesi\\PairModel\\wandb\\run-20231018_155108-evi918yp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pair_fragments/Modello_ottimizzato/runs/evi918yp' target=\"_blank\">comfy-eon-1</a></strong> to <a href='https://wandb.ai/pair_fragments/Modello_ottimizzato' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pair_fragments/Modello_ottimizzato' target=\"_blank\">https://wandb.ai/pair_fragments/Modello_ottimizzato</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pair_fragments/Modello_ottimizzato/runs/evi918yp' target=\"_blank\">https://wandb.ai/pair_fragments/Modello_ottimizzato/runs/evi918yp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"Modello_ottimizzato\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      notes = \"modello ottimizzato senza cicli for \",\n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "      \"learning_rate\": 0.0001,\n",
    "      \"architecture\": \"Model2_mod\",\n",
    "      \"epochs\": 10,\n",
    "      \"weight_decay\": 0.00005,\n",
    "      \"W_crossentropy\":1,\n",
    "      \"W_contrastive\":0,\n",
    "      \"type_of_couples\": 1,\n",
    "      \"num_of batch\" : len(train_loader),\n",
    "      \"seed\": seed,\n",
    "      \"name_saved\":'vff'  \n",
    "      })\n",
    "      \n",
    "config = wandb.config\n",
    "\n",
    "# Calcola i pesi delle classi in base alle percentuali\n",
    "#weight_class_0 = 1.0 / 0.7  # Peso per la classe 0\n",
    "#weight_class_1 = 1.0 / 0.3  # Peso per la classe 1\n",
    "\n",
    "# Crea un tensore PyTorch con i pesi\n",
    "#weight = torch.tensor([weight_class_0, weight_class_1], dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "contrast_criterion = ContrastiveLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "num_epochs = config.epochs\n",
    "best_val_accuracy = 0.0 \n",
    "\n",
    "\n",
    "\n",
    "#miner = miners.MultiSimilarityMiner()\n",
    "#loss_func = losses.TripletMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1348 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.4839, Train Accuracy: 0.7951, Val Loss: 0.5187, Val Accuracy: 0.7845, Val Loss unbalanced: 1.1759, Val Accuracy unbalanced: 0.4966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.4719, Train Accuracy: 0.8029, Val Loss: 0.5180, Val Accuracy: 0.7715, Val Loss unbalanced: 1.1938, Val Accuracy unbalanced: 0.5357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.4653, Train Accuracy: 0.8051, Val Loss: 0.5286, Val Accuracy: 0.7883, Val Loss unbalanced: 1.3301, Val Accuracy unbalanced: 0.4953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.4593, Train Accuracy: 0.8066, Val Loss: 0.5163, Val Accuracy: 0.7772, Val Loss unbalanced: 1.2454, Val Accuracy unbalanced: 0.5262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.4517, Train Accuracy: 0.8110, Val Loss: 0.5237, Val Accuracy: 0.7848, Val Loss unbalanced: 1.3264, Val Accuracy unbalanced: 0.5101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.4440, Train Accuracy: 0.8131, Val Loss: 0.5364, Val Accuracy: 0.7747, Val Loss unbalanced: 1.4049, Val Accuracy unbalanced: 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alessandro\\Desktop\\Tesi\\PairModel\\Pair_model_V4.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/Pair_model_V4.ipynb#X31sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/Pair_model_V4.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#7 predict the y \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/Pair_model_V4.ipynb#X31sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(cluster_outputs_\u001b[39m.\u001b[39;49mdata, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/Pair_model_V4.ipynb#X31sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m train_of_results\u001b[39m.\u001b[39mappend(predicted\u001b[39m.\u001b[39mtolist()) \u001b[39m# append the predicted y\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/Pair_model_V4.ipynb#X31sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m one_hot_labels_\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_dir = r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\Check_points'\n",
    "results_dir = r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\results'\n",
    "\n",
    "checkpoint_interval = 1  \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    list_of_results=[]\n",
    "    list_of_true=[]\n",
    "\n",
    "    train_of_results=[]\n",
    "    train_of_true=[]\n",
    "\n",
    "    ######################################################################################\n",
    "    ##                                      Train                                       ##\n",
    "    ######################################################################################\n",
    "    model.train() \n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "    for batch_data in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Each cluster is composed by an adjacency matrix and the cluster of fragments\n",
    "        matrix, cluster = batch_data\n",
    "        \n",
    "        #1 Let's convert the matrix into an y list and count the number of elements in the matrix\n",
    "        matrix = matrix.numpy()\n",
    "        ind=torch.triu_indices(len(matrix[0]),len(matrix[0]),offset =1)\n",
    "        y=matrix[0][ind[0],ind[1]]\n",
    "        n_frag_ = len(matrix[0])\n",
    "\n",
    "        #2 select the positions and the relative labels that represent the pairs we want to extract\n",
    "        positions, labels_ = select_couples(y,config.type_of_couples)\n",
    "        labels_ = labels_.long().to(device)\n",
    "        one_hot_labels_ = F.one_hot(labels_,2)\n",
    "        train_of_true.append(labels_.tolist()) # append the true y\n",
    "\n",
    "\n",
    "        #3 apply translation to the origin and random rotations\n",
    "        cluster = apply_translation(cluster[0])\n",
    "        cluster = apply_randomrotations(cluster)\n",
    "\n",
    "        #4 input data to the model\n",
    "        cluster_outputs_, frags_a, frags_b = model(cluster, positions, n_frag_)\n",
    "        \n",
    "        #5 calculate the cross entropy and the contrastive  losses\n",
    "        loss_ = criterion(cluster_outputs_, one_hot_labels_.float())\n",
    "        contrast_loss = contrast_criterion(frags_a, frags_b, labels_)\n",
    "        loss = loss_ + config.W_contrastive*contrast_loss\n",
    "\n",
    "        #6 backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #7 predict the y \n",
    "        _, predicted = torch.max(cluster_outputs_.data, 1)\n",
    "        train_of_results.append(predicted.tolist()) # append the predicted y\n",
    "        total_samples += one_hot_labels_.size(0)\n",
    "        correct_predictions += (predicted == labels_).sum().item()\n",
    "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions / total_samples}) # the progress bar tha shows for each cluster the live metrics\n",
    "   \n",
    "    #8 compute the metrics at the end of each epoch\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    train_loss = total_loss/len(train_loader)\n",
    "    \n",
    "    #9 log the metrics and the model of the current epoch\n",
    "    metrics_train = {\"train_loss\": train_loss, \n",
    "                       \"accuracy\": accuracy}\n",
    "    wandb.log(metrics_train)\n",
    "    \n",
    "    current_time = datetime.datetime.now()\n",
    "    checkpoint_name = f\"{current_time.strftime('%d_%m_%H%M')}_{epoch+1}_{config.name_saved}.pt\"    #################\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    wandb.run.log_artifact(checkpoint_path, name=checkpoint_name)\n",
    "\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    ##                                    Inference                                     ##\n",
    "    ######################################################################################\n",
    " \n",
    "    #                           #\n",
    "    # COPPIE BILANCIATE E FISSE #\n",
    "    #                           # \n",
    "\n",
    "    model.eval()  \n",
    "    \n",
    "    val_loss_ = 0.0\n",
    "    v_contrast_loss= 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "\n",
    "            val_matrix, val_cluster = val_batch\n",
    "\n",
    "            val_matrix = val_matrix.numpy()\n",
    "            ind_val=torch.triu_indices(len(val_matrix[0]),len(val_matrix[0]),offset =1)\n",
    "            y_val =val_matrix[0][ind_val[0],ind_val[1]]\n",
    "            n_frag_val = len(val_matrix[0])\n",
    "\n",
    "            # select the same number of 0 and 1 in the previous vector \n",
    "            val_positions,  val_labels_ = select_couples(y_val,1)\n",
    "            val_labels_ = val_labels_.long().to(device)\n",
    "            one_hot_labels_val_ = F.one_hot(val_labels_,2)\n",
    "            list_of_true.append(val_labels_.tolist())\n",
    "\n",
    "            val_cluster = apply_translation(val_cluster[0])\n",
    "            val_cluster = apply_randomrotations(val_cluster)\n",
    "            val_outputs_,  v_frags_a, v_frags_b = model(val_cluster,  val_positions, n_frag_val)\n",
    "            \n",
    "            val_loss_ += criterion(val_outputs_, one_hot_labels_val_.float()).item()\n",
    "            v_contrast_loss += contrast_criterion(v_frags_a, v_frags_b, val_labels_).item() \n",
    "           \n",
    "            val_loss = val_loss_ + config.W_contrastive*v_contrast_loss\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs_.data, 1)\n",
    "            list_of_results.append(val_predicted.tolist())\n",
    "            val_total_samples += one_hot_labels_val_.size(0)\n",
    "            val_correct_predictions += (val_predicted == val_labels_).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct_predictions / val_total_samples\n",
    "        val_loss /= len(val_loader)\n",
    "    \n",
    "    val_metrics = {\"val_loss\": val_loss, \n",
    "                       \"val_accuracy\": val_accuracy}\n",
    "    wandb.log(val_metrics)  \n",
    "\n",
    "\n",
    "    #                  #\n",
    "    # TUTTE LE COPPIE  #\n",
    "    #                  # \n",
    "\n",
    "    model.eval()  \n",
    "    \n",
    "    val_loss_ = 0.0\n",
    "    v_contrast_loss= 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "\n",
    "            val_matrix, val_cluster = val_batch\n",
    "\n",
    "            val_matrix = val_matrix.numpy()\n",
    "            ind_val=torch.triu_indices(len(val_matrix[0]),len(val_matrix[0]),offset =1)\n",
    "            y_val =val_matrix[0][ind_val[0],ind_val[1]]\n",
    "            n_frag_val = len(val_matrix[0])\n",
    "\n",
    "            # select the same number of 0 and 1 in the previous vector \n",
    "            val_positions,  val_labels_ = select_couples(y_val,0)\n",
    "            val_labels_ = torch.Tensor(val_labels_)\n",
    "            val_labels_ = val_labels_.long().to(device)\n",
    "            one_hot_labels_val_ = F.one_hot(val_labels_,2)\n",
    "            list_of_true.append(val_labels_.tolist())\n",
    "\n",
    "            val_cluster = apply_translation(val_cluster[0])\n",
    "            val_cluster = apply_randomrotations(val_cluster)\n",
    "            val_outputs_,  v_frags_a, v_frags_b = model(val_cluster,  val_positions, n_frag_val)\n",
    "            \n",
    "            val_loss_ += criterion(val_outputs_, one_hot_labels_val_.float()).item()\n",
    "            v_contrast_loss += contrast_criterion(v_frags_a, v_frags_b, val_labels_).item() \n",
    "           \n",
    "            val_loss_all = val_loss_ + config.W_contrastive*v_contrast_loss\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs_.data, 1)\n",
    "            list_of_results.append(val_predicted.tolist())\n",
    "            val_total_samples += one_hot_labels_val_.size(0)\n",
    "            val_correct_predictions += (val_predicted == val_labels_).sum().item()\n",
    "\n",
    "        val_accuracy_all = val_correct_predictions / val_total_samples\n",
    "        val_loss_all /= len(val_loader)\n",
    "    \n",
    "    all_the_values = {\"val_loss_all\": val_loss_all, \n",
    "                       \"val_accuracy_all\": val_accuracy_all}\n",
    "\n",
    "    wandb.log(all_the_values)                   \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {accuracy:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Loss unbalanced: {val_loss_all:.4f}, Val Accuracy unbalanced: {val_accuracy_all:.4f}')\n",
    "\n",
    "###################################################### STORE THE RESULTS ###############################################################################################################\n",
    "\n",
    "    result_path = os.path.join(results_dir, f\"VAL+_{config.name_saved}_{epoch+1}\")              #############################\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(result_path, \"list_of_true.pkl\"), 'wb') as f:\n",
    "        pickle.dump(list_of_true, f)\n",
    "\n",
    "    with open(os.path.join(result_path, \"list_of_results.pkl\"), 'wb') as f:\n",
    "        pickle.dump(list_of_results, f)\n",
    "\n",
    "    train_result_path = os.path.join(results_dir, f\"TRAIN+_{config.name_saved}_{epoch+1}\")             ##################  \n",
    "    os.makedirs(train_result_path, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(train_result_path, \"train_of_true.pkl\" ), 'wb') as f:\n",
    "        pickle.dump(train_of_true, f)\n",
    "\n",
    "    with open(os.path.join(train_result_path, \"train_of_results.pkl\"), 'wb') as f:\n",
    "        pickle.dump(train_of_results, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▄▁</td></tr><tr><td>train_loss</td><td>▁▅█</td></tr><tr><td>val_accuracy</td><td>█▁▁</td></tr><tr><td>val_accuracy_all</td><td>▇█▁</td></tr><tr><td>val_loss</td><td>▁█▇</td></tr><tr><td>val_loss_all</td><td>█▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.66253</td></tr><tr><td>train_loss</td><td>0.63715</td></tr><tr><td>val_accuracy</td><td>0.68106</td></tr><tr><td>val_accuracy_all</td><td>0.42655</td></tr><tr><td>val_loss</td><td>0.63491</td></tr><tr><td>val_loss_all</td><td>0.86914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-meadow-4</strong> at: <a href='https://wandb.ai/pair_fragments/Tentativo_nuovo/runs/l0tdeula' target=\"_blank\">https://wandb.ai/pair_fragments/Tentativo_nuovo/runs/l0tdeula</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231017_154701-l0tdeula\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.5223, Train Accuracy: 0.7734, Val Loss: 285.9390, Val Accuracy: 0.7573, Val Loss unbalanced: 0.9756, Val Accuracy unbalanced: 0.5558\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "    \n",
    "val_loss_ = 0.0\n",
    "v_contrast_loss= 0.0\n",
    "val_correct_predictions = 0\n",
    "val_total_samples = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "\n",
    "            val_matrix, val_cluster = val_batch\n",
    "\n",
    "            val_matrix = val_matrix.numpy()\n",
    "            ind_val=torch.triu_indices(len(val_matrix[0]),len(val_matrix[0]),offset =1)\n",
    "            y_val =val_matrix[0][ind_val[0],ind_val[1]]\n",
    "            n_frag_val = len(val_matrix[0])\n",
    "\n",
    "            # select the same number of 0 and 1 in the previous vector \n",
    "            val_positions,  val_labels_ = select_couples(y_val,0)\n",
    "            val_labels_ = torch.Tensor(val_labels_)\n",
    "            val_labels_ = val_labels_.long().to(device)\n",
    "            one_hot_labels_val_ = F.one_hot(val_labels_,2)\n",
    "            list_of_true.append(val_labels_.tolist())\n",
    "\n",
    "            val_cluster = apply_translation(val_cluster[0])\n",
    "            val_cluster = apply_randomrotations(val_cluster)\n",
    "            val_outputs_,  v_frags_a, v_frags_b = model(val_cluster,  val_positions, n_frag_val)\n",
    "            \n",
    "            val_loss_ += criterion(val_outputs_, one_hot_labels_val_.float()).item()\n",
    "            v_contrast_loss += contrast_criterion(v_frags_a, v_frags_b, val_labels_).item() \n",
    "           \n",
    "            val_loss_all = val_loss_ + config.W_contrastive*v_contrast_loss\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs_.data, 1)\n",
    "            list_of_results.append(val_predicted.tolist())\n",
    "            val_total_samples += one_hot_labels_val_.size(0)\n",
    "            val_correct_predictions += (val_predicted == val_labels_).sum().item()\n",
    "\n",
    "        val_accuracy_all = val_correct_predictions / val_total_samples\n",
    "        val_loss_all  /= len(val_loader)\n",
    "\n",
    "    \n",
    "all_the_values = {\"val_loss_all\": val_loss_all, \n",
    "                       \"val_accuracy_all\": val_accuracy_all}\n",
    "\n",
    "wandb.log(all_the_values)                   \n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {accuracy:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Loss unbalanced: {val_loss_all:.4f}, Val Accuracy unbalanced: {val_accuracy_all:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(results_dir, f\"VAL+_{config.name_saved}_{epoch+1}\")              #############################\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(result_path, \"list_of_true.pkl\"), 'wb') as f:\n",
    "        pickle.dump(list_of_true, f)\n",
    "\n",
    "with open(os.path.join(result_path, \"list_of_results.pkl\"), 'wb') as f:\n",
    "        pickle.dump(list_of_results, f)\n",
    "\n",
    "train_result_path = os.path.join(results_dir, f\"TRAIN+_{config.name_saved}_{epoch+1}\")             ##################  \n",
    "os.makedirs(train_result_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(train_result_path, \"train_of_true.pkl\" ), 'wb') as f:\n",
    "        pickle.dump(train_of_true, f)\n",
    "\n",
    "with open(os.path.join(train_result_path, \"train_of_results.pkl\"), 'wb') as f:\n",
    "        pickle.dump(train_of_results, f)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
