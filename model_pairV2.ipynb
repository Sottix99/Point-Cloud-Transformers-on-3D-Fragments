{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import special_ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_GPU():\n",
    "    \"\"\" This function activates the gpu \n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(torch.cuda.get_device_name(0), \"is available and being used\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available, using CPU instead\") \n",
    "    return device  \n",
    "\n",
    "\n",
    "def center_in_origin(frag):\n",
    "    \"\"\" This function normalize each fragment in (0,1)\n",
    "    \"\"\"\n",
    "    min_vals, _ = torch.min(frag[:, 0:3], axis=0)\n",
    "    max_vals, _ = torch.max(frag[:, 0:3], axis=0)\n",
    "    frag[:, 0:3] = (frag[:, 0:3] - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    return frag\n",
    "    \n",
    "def normalize(batch):\n",
    "    \"\"\" This function apply center_in_origin() to each fragment in the batch\n",
    "    \"\"\"\n",
    "    out=[]\n",
    "    for element in batch:\n",
    "        out.append(center_in_origin(element))\n",
    "    out_tensor = torch.stack(out)    \n",
    "    return out_tensor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code form the repository of PCT https://github.com/qq456cvb/Point-Transformers\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zmï¼›\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    return torch.sum((src[:, :, None] - dst[:, None]) ** 2, dim=-1)\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S, [K]]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, [K], C]\n",
    "    \"\"\"\n",
    "    raw_size = idx.size()\n",
    "    idx = idx.reshape(raw_size[0], -1)\n",
    "    res = torch.gather(points, 1, idx[..., None].expand(-1, -1, points.size(-1)))\n",
    "    return res.reshape(*raw_size, -1)\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        distance = torch.min(distance, dist)\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def sample_and_group(npoint, nsample, xyz, points):\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint \n",
    "    \n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint]\n",
    "\n",
    "    new_xyz = index_points(xyz, fps_idx) \n",
    "    new_points = index_points(points, fps_idx)\n",
    "\n",
    "    dists = square_distance(new_xyz, xyz)  # B x npoint x N\n",
    "    idx = dists.argsort()[:, :, :nsample]  # B x npoint x K\n",
    "\n",
    "    grouped_points = index_points(points, idx)\n",
    "    grouped_points_norm = grouped_points - new_points.view(B, S, 1, -1)\n",
    "    new_points = torch.cat([grouped_points_norm, new_points.view(B, S, 1, -1).repeat(1, 1, nsample, 1)], dim=-1)\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class Local_op(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, s, d = x.size()  # torch.Size([32, 512, 32, 6]) \n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.reshape(-1, d, s)\n",
    "        batch_size, _, N = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x))) # B, D, N\n",
    "        x = torch.max(x, 2)[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = x.reshape(b, n, -1).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SA_Layer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.q_conv = nn.Conv1d(channels, channels // 4, 1, bias=False)\n",
    "        self.k_conv = nn.Conv1d(channels, channels // 4, 1, bias=False)\n",
    "        self.q_conv.weight = self.k_conv.weight \n",
    "        self.v_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.trans_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.after_norm = nn.BatchNorm1d(channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_q = self.q_conv(x).permute(0, 2, 1) # b, n, c \n",
    "        x_k = self.k_conv(x)# b, c, n        \n",
    "        x_v = self.v_conv(x)\n",
    "        energy = x_q @ x_k # b, n, n \n",
    "        attention = self.softmax(energy)\n",
    "        attention = attention / (1e-9 + attention.sum(dim=1, keepdims=True))\n",
    "        x_r = x_v @ attention # b, c, n \n",
    "        x_r = self.act(self.after_norm(self.trans_conv(x - x_r)))\n",
    "        x = x + x_r\n",
    "        return x\n",
    "    \n",
    "\n",
    "class StackedAttention(nn.Module):\n",
    "    def __init__(self, channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "\n",
    "        self.sa1 = SA_Layer(channels)\n",
    "        self.sa2 = SA_Layer(channels)\n",
    "        self.sa3 = SA_Layer(channels)\n",
    "        self.sa4 = SA_Layer(channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # \n",
    "        # b, 3, npoint, nsample  \n",
    "        # conv2d 3 -> 128 channels 1, 1\n",
    "        # b * npoint, c, nsample \n",
    "        # permute reshape\n",
    "        batch_size, _, N = x.size()\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x1 = self.sa1(x)\n",
    "        x2 = self.sa2(x1)\n",
    "        x3 = self.sa3(x2)\n",
    "        x4 = self.sa4(x3)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_points = 7 # we have 7 features for each point\n",
    "        self.conv1 = nn.Conv1d(d_points, 64, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.gather_local_0 = Local_op(in_channels=128, out_channels=128)\n",
    "        self.gather_local_1 = Local_op(in_channels=256, out_channels=256)\n",
    "        self.pt_last = StackedAttention()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv_fuse = nn.Sequential(nn.Conv1d(1280, 1024, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm1d(1024),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xyz = x[..., :3]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        batch_size, _, _ = x.size()\n",
    "        x= x.double()\n",
    "        x = self.relu(self.bn1(self.conv1(x))) # B, D, N\n",
    "        x = self.relu(self.bn2(self.conv2(x))) # B, D, N\n",
    "        x = x.permute(0, 2, 1)\n",
    "        new_xyz, new_feature = sample_and_group(npoint=512, nsample=32, xyz=xyz, points=x)         \n",
    "        feature_0 = self.gather_local_0(new_feature)\n",
    "        feature = feature_0.permute(0, 2, 1)\n",
    "        new_xyz, new_feature = sample_and_group(npoint=256, nsample=32, xyz=new_xyz, points=feature) \n",
    "        feature_1 = self.gather_local_1(new_feature)\n",
    "        \n",
    "        x = self.pt_last(feature_1)\n",
    "        x = torch.cat([x, feature_1], dim=1)\n",
    "        x = self.conv_fuse(x)\n",
    "        x = torch.max(x, 2)[0] # Returns the maximum value of all elements in the input tensor. (2 elementes for each vector)\n",
    "        x = x.view(batch_size, -1) # Returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "train = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\train_pair_dataset_REG.pt\")\n",
    "val = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\val_pair_dataset_REG.pt\")\n",
    "#test = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\test_pair_dataset_REG.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions to remove (Train): [2, 9, 13, 14, 17, 20, 22, 28, 30, 31, 34, 35, 36, 39, 51, 54, 55, 57, 60, 62, 63, 71, 78, 87, 90, 91, 95, 109, 128, 136, 148, 152, 161, 163, 164, 167, 192, 197, 205, 209, 210, 215, 217, 218, 222, 225, 228, 232, 241, 255, 261, 263, 265, 273, 276, 284, 299, 300, 303, 308, 318, 325, 330, 337, 339, 344, 346, 351, 353, 354, 361, 364, 369, 379, 385, 398, 400, 405, 412, 420, 423, 427, 431, 433, 441, 445, 447, 455, 468, 469, 471, 474, 489, 494, 496, 506, 507, 513, 516, 517, 520, 534, 538, 547, 551, 557, 564, 571, 577, 582, 591, 593, 595, 597, 600, 606, 607, 613, 617, 633, 635, 644, 651, 652, 660, 664, 667, 673, 675, 680, 689, 694, 699, 700, 701, 703, 707, 711, 721, 731, 735, 739, 742, 751, 754, 757, 758, 763, 768, 771, 776, 778, 782, 783, 786, 789, 791, 813, 818, 823, 831, 834, 837, 841, 844, 846, 847, 850, 874, 876, 879, 880, 896, 909, 917, 926, 931, 934, 942, 943, 960, 967, 971, 977, 979, 986, 988, 991, 994, 996, 999, 1000, 1006, 1007, 1012, 1019, 1020, 1027, 1038, 1039, 1042, 1045, 1050, 1052, 1055, 1057, 1067, 1077, 1078, 1079, 1082, 1084, 1100, 1103, 1109, 1117, 1118, 1126, 1127, 1129, 1130, 1131, 1136, 1138, 1139, 1143, 1150, 1153, 1155, 1156, 1160, 1188, 1193, 1203, 1217, 1219, 1227, 1243, 1254, 1262, 1267, 1274, 1284, 1290, 1295, 1296, 1299, 1303, 1304, 1309, 1316, 1319, 1324, 1327, 1330, 1334, 1336, 1337, 1347, 1350, 1353, 1356, 1358, 1360, 1364, 1374, 1390, 1392, 1393, 1394, 1399, 1400, 1412, 1418, 1441, 1460, 1462, 1463, 1466, 1469, 1470, 1473, 1474, 1483, 1489, 1494, 1498, 1499, 1504, 1505, 1507, 1514]\n",
      "Positions to remove (Val): [6, 12, 13, 22, 24, 31, 46, 47, 54, 65, 70, 74, 77, 94, 95, 105, 112, 113, 114, 118, 125, 136, 137, 140, 142, 145, 146, 147, 151, 157, 160, 162, 163, 166, 169, 173, 177, 180, 183, 187, 191, 192, 196, 205, 216, 218, 219, 223, 225, 226, 241, 255, 256, 257, 281, 288, 289, 293, 294, 300, 301, 308, 309, 314, 322]\n"
     ]
    }
   ],
   "source": [
    "# let's find the largest clusters\n",
    "\n",
    "count = 0\n",
    "indices = []\n",
    "count_val = 0\n",
    "indices_val = []\n",
    "\n",
    "for i in range(0, 1526):\n",
    "    if train[i][0].shape[0] > 60:\n",
    "        count += 1\n",
    "        indices.append(i)\n",
    "\n",
    "for i in range(0, val.shape[0]):\n",
    "    if val[i][0].shape[0] > 60:\n",
    "        count_val += 1\n",
    "        indices_val.append(i)\n",
    "\n",
    "print(\"Positions to remove (Train):\", indices)\n",
    "print(\"Positions to remove (Val):\", indices_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are removing the largest clusters for computational reasons\n",
    "mask = torch.ones(train.shape[0], dtype=torch.bool)\n",
    "mask[indices] = False\n",
    "filtered_tensor = train[mask]\n",
    "train = filtered_tensor\n",
    "\n",
    "mask_val = torch.ones(val.shape[0], dtype=torch.bool)\n",
    "mask_val[indices_val] = False\n",
    "filtered_tensor_val = val[mask_val]\n",
    "val = filtered_tensor_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1234, 2)\n",
      "(262, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjmatrix_into_y(adj_mat):\n",
    "\n",
    "    \"\"\" This function maps the adj_matrix into a vector (leverage the matrix symmetry to save computations)\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    \n",
    "    # Discover the number of fragments\n",
    "    n_frags = len(adj_mat[0])\n",
    "    \n",
    "    for j in range(0, n_frags - 1):\n",
    "        init = j + 1\n",
    "        for k in range(init, n_frags):\n",
    "            \n",
    "            labels.append(adj_mat[j][k])\n",
    "    \n",
    "    return  labels\n",
    "\n",
    "def max_the1_v (vector):\n",
    "\n",
    "    \"\"\"  This function outputs a vector containing the positions of fragment pairs to extract in order to ensure generating the maximum possible number of balanced pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    how_many_ones = 0\n",
    "    how_many_zeroes = 0\n",
    "\n",
    "    positions_0 = []\n",
    "    positions_1 = []\n",
    " \n",
    "\n",
    "    for i, elemento in enumerate(vector):\n",
    "        if elemento == 1 :\n",
    "            how_many_ones += 1\n",
    "            positions_1.append(i)\n",
    "\n",
    "\n",
    "        elif elemento == 0 :\n",
    "            how_many_zeroes += 1 \n",
    "            positions_0.append(i)\n",
    "    \n",
    "    # find the min = k \n",
    "    selected_value=min(how_many_ones, how_many_zeroes)\n",
    "\n",
    "    #  select only the first k elements from each vector\n",
    "    positions_1_sample = positions_1[:selected_value]\n",
    "    positions_0_sample = positions_0[:selected_value]\n",
    "\n",
    "    posizioni =   positions_1_sample + positions_0_sample\n",
    "    posizioni = sorted(posizioni)  \n",
    "    return posizioni\n",
    "\n",
    "\n",
    "def CreateCouples_intra_train_masked_pos(cluster_of_pt, adj_mat, posizioni):\n",
    "\n",
    "    \"\"\" This function is used to create the couples from the cluster in which each element is already preprocessed from the pct. \n",
    "        It doesn't create all possible pairs; in fact, by using the position vector, we know in which positions the pairs we need to extract are located\n",
    "        (those that allow us to have the maximum number of balanced pairs within the cluster).\n",
    "    \n",
    "    Input:\n",
    "        cluster_of_pt: the cluster of pointcloud (alredy processed from the PCT)\n",
    "        adj_mat: the adjacency matrix associated to the cluster\n",
    "        posizioni: the vector of positions obatined from max_the1_v() function\n",
    "    Return:\n",
    "        frag_a: the tensor of the first element of each couple\n",
    "        frag_b: the tensor of the second element of each couple\n",
    "        labels: a binary value that indicates if the couple is adjacent (1) or not (0)\n",
    "    \"\"\"\n",
    "    \n",
    "    frag_a = []\n",
    "    frag_b = []\n",
    "    labels = []\n",
    "    pos_realtime=0\n",
    "    # Discover the number of fragments\n",
    "    n_frags = len(adj_mat[0])\n",
    "    \n",
    "    for j in range(0, n_frags - 1):\n",
    "        init = j + 1\n",
    "        \n",
    "\n",
    "        for k in range(init, n_frags):\n",
    "            if pos_realtime in posizioni:  # Tests whether pos_realtime is contained in positions\n",
    "                frag_a_tensor = cluster_of_pt[j]\n",
    "                frag_b_tensor = cluster_of_pt[k]\n",
    "                label_value = adj_mat[j][k]\n",
    "\n",
    "                # Stack of tensors and labels at each step\n",
    "                if len(frag_a) == 0:\n",
    "                    frag_a = frag_a_tensor.unsqueeze(0)\n",
    "                    frag_b = frag_b_tensor.unsqueeze(0)\n",
    "                    labels = torch.tensor([label_value])\n",
    "                else:\n",
    "                    frag_a = torch.cat((frag_a, frag_a_tensor.unsqueeze(0)), dim=0)\n",
    "                    frag_b = torch.cat((frag_b, frag_b_tensor.unsqueeze(0)), dim=0)\n",
    "                    labels = torch.cat((labels, torch.tensor([label_value])), dim=0)\n",
    "\n",
    "            pos_realtime = pos_realtime + 1\n",
    "    \n",
    "    return frag_a, frag_b, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptc_net = Branch()\n",
    "        self.pair_net = PairModel2()      \n",
    "\n",
    "    def forward(self, matrix, cluster):\n",
    "        matrix = matrix.numpy()\n",
    "        # let's convert the matrix into a vector\n",
    "        y = adjmatrix_into_y(matrix[0])\n",
    "        # select the same number of 0 and 1 in the previous vector\n",
    "        positions = max_the1_v(y)\n",
    "        # normalize each element of the cluster\n",
    "        cluster = normalize(cluster[0])\n",
    "        cluster = cluster.double().to(device)\n",
    "\n",
    "        # create an empty tensor that will collect the trasformed fragments\n",
    "        cluster_transformed = torch.Tensor([]).to(device)\n",
    "\n",
    "        sub_cluster_start = 0\n",
    "        while sub_cluster_start < len(matrix[0][0]):\n",
    "            sub_cluster_end = min(sub_cluster_start + 2, len(matrix[0][0]))\n",
    "            cluster_subset = cluster[sub_cluster_start:sub_cluster_end]\n",
    "            cluster_subset = cluster_subset.to(device)\n",
    "            # apply the PCT\n",
    "            point_clouds_transformed = self.ptc_net(cluster_subset)\n",
    "            # append the results\n",
    "            cluster_transformed = torch.cat((cluster_transformed, point_clouds_transformed), dim=0)\n",
    "            sub_cluster_start += 2\n",
    "        \n",
    "        # given the positions, the matrix and the transformed cluster create the couples\n",
    "        frags_a, frags_b, labels = CreateCouples_intra_train_masked_pos(cluster_transformed, matrix[0], positions)\n",
    "        frags_a = frags_a.double().to(device)\n",
    "        frags_b = frags_b.double().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        n_couples = frags_a.shape[0]\n",
    "\n",
    "        # create an empty tensor that will collect the output of the classificator\n",
    "        cluster_outputs = torch.Tensor([]).to(device)\n",
    "\n",
    "        el = 0\n",
    "\n",
    "        while el < n_couples:\n",
    "            end_idx = min(el + 16, n_couples)\n",
    "            batch_frags_a = frags_a[el:end_idx]\n",
    "            batch_frags_b = frags_b[el:end_idx]\n",
    "            # apply the classifier on a batch of 16 couples\n",
    "            outputs = self.pair_net(batch_frags_a, batch_frags_b)\n",
    "            # append the outputs\n",
    "            cluster_outputs = torch.cat((cluster_outputs, outputs), dim=0)\n",
    "            \n",
    "            el += 16\n",
    "\n",
    "        return cluster_outputs, labels\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class PairModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        output_channels = 2 # it's a binary classification\n",
    "\n",
    "      \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "        # classificator\n",
    "        self.linear0 = nn.Linear(2048, 1024, bias=False)\n",
    "        self.bn0 = nn.BatchNorm1d(1024)\n",
    "        self.dp0 = nn.Dropout(p=0.5)\n",
    "        self.linear1 = nn.Linear(1024, 512, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=0.3)\n",
    "        self.linear3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dp3 = nn.Dropout(p=0.3)\n",
    "        self.linear4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dp4 = nn.Dropout(p=0.3)\n",
    "        self.linear5 = nn.Linear(64, output_channels)\n",
    "        \n",
    "    def forward(self, x_1, x_2):\n",
    "       \n",
    "        x_mult = x_1 * x_2 # sum the two elements of the couples\n",
    "        x_sum = x_1 * x_2 # multiply the two elements of the couples\n",
    "        x = torch.cat((x_mult, x_sum), dim = 1) \n",
    "\n",
    "        # classificator\n",
    "        if x_1.shape[0] > 1:\n",
    "            x = self.relu(self.bn0(self.linear0(x)))\n",
    "            #x = self.dp0(x)\n",
    "            x = self.relu(self.bn1(self.linear1(x)))\n",
    "            x = self.dp1(x)\n",
    "            x = self.relu(self.bn2(self.linear2(x)))\n",
    "            x = self.dp2(x)\n",
    "            x = self.relu(self.bn3(self.linear3(x)))\n",
    "            x = self.dp3(x)\n",
    "            x = self.relu(self.bn4(self.linear4(x)))\n",
    "            x = self.dp4(x)\n",
    "            x = self.linear5(x)\n",
    "\n",
    "        # If we have only one pair, applying batch normalization doesn't make sense.\n",
    "        else: \n",
    "            x = self.relu(self.linear0(x))\n",
    "            #x = self.dp0(x)\n",
    "            x = self.relu(self.linear1(x))\n",
    "            x = self.dp1(x)\n",
    "            x = self.relu(self.linear2(x))\n",
    "            x = self.dp2(x)\n",
    "            x = self.relu(self.linear3(x))\n",
    "            x = self.dp3(x)\n",
    "            x = self.relu(self.linear4(x))\n",
    "            x = self.dp4(x)\n",
    "            x = self.linear5(x)\n",
    "                    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4080 is available and being used\n"
     ]
    }
   ],
   "source": [
    "device=use_GPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.tolist()\n",
    "val = val.tolist()\n",
    "# We process one cluster at a time.\n",
    "train_loader = DataLoader(train, batch_size=1)\n",
    "val_loader = DataLoader(val, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2(\n",
       "  (ptc_net): Branch(\n",
       "    (conv1): Conv1d(7, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (gather_local_0): Local_op(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (gather_local_1): Local_op(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pt_last): StackedAttention(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sa1): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa2): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa3): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa4): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (conv_fuse): Sequential(\n",
       "      (0): Conv1d(1280, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (pair_net): PairModel2(\n",
       "    (relu): ReLU()\n",
       "    (linear0): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "    (bn0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp0): Dropout(p=0.5, inplace=False)\n",
       "    (linear1): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp1): Dropout(p=0.2, inplace=False)\n",
       "    (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp2): Dropout(p=0.3, inplace=False)\n",
       "    (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp3): Dropout(p=0.3, inplace=False)\n",
       "    (linear4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dp4): Dropout(p=0.3, inplace=False)\n",
       "    (linear5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model2().to(device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.6939, Training Accuracy: 0.5218, Validation Loss: 0.7104, Validation Accuracy: 0.4583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Loss: 0.6789, Training Accuracy: 0.5839, Validation Loss: 0.6977, Validation Accuracy: 0.4856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Training Loss: 0.6602, Training Accuracy: 0.6330, Validation Loss: 0.7073, Validation Accuracy: 0.4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Training Loss: 0.6385, Training Accuracy: 0.6781, Validation Loss: 0.7194, Validation Accuracy: 0.4267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Training Loss: 0.6175, Training Accuracy: 0.7086, Validation Loss: 0.7171, Validation Accuracy: 0.4924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Training Loss: 0.6035, Training Accuracy: 0.7260, Validation Loss: 0.7223, Validation Accuracy: 0.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Training Loss: 0.6041, Training Accuracy: 0.7246, Validation Loss: 0.7414, Validation Accuracy: 0.4634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Training Loss: 0.5962, Training Accuracy: 0.7369, Validation Loss: 0.7400, Validation Accuracy: 0.4301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Training Loss: 0.5939, Training Accuracy: 0.7372, Validation Loss: 0.7807, Validation Accuracy: 0.4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Training Loss: 0.5927, Training Accuracy: 0.7363, Validation Loss: 0.7661, Validation Accuracy: 0.4503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\Alessandro\\Desktop\\Tesi\\PairModel\\model_pair.ipynb Cell 60\u001b[0m line \u001b[0;36m3\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/model_pair.ipynb#Y112sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m one_hot_labels_ \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(labels_,\u001b[39m2\u001b[39m)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/model_pair.ipynb#Y112sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(cluster_outputs_, one_hot_labels_\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/model_pair.ipynb#Y112sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/model_pair.ipynb#Y112sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Alessandro/Desktop/Tesi/PairModel/model_pair.ipynb#Y112sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Alessandro\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n",
      "\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n",
      "\u001b[0;32m    486\u001b[0m     )\n",
      "\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n",
      "\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n",
      "\u001b[0;32m    489\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Alessandro\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n",
      "\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n",
      "\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "num_epochs = 20\n",
    "best_val_accuracy = 0.0 \n",
    "\n",
    "checkpoint_dir = r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\Check_points'\n",
    "\n",
    "\n",
    "checkpoint_interval = 1  \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "    ###########\n",
    "    ## Train ##\n",
    "    ###########\n",
    "\n",
    "\n",
    "    for batch_data in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        matrix, cluster = batch_data\n",
    "\n",
    "        cluster_outputs_, labels_  = model(matrix, cluster)\n",
    "\n",
    "        one_hot_labels_ = F.one_hot(labels_,2)\n",
    "        loss = criterion(cluster_outputs_, one_hot_labels_.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(cluster_outputs_.data, 1)\n",
    "        total_samples += one_hot_labels_.size(0)\n",
    "        correct_predictions += (predicted == labels_).sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions / total_samples})\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    train_loss = total_loss/len(train_loader)\n",
    "\n",
    "    ###############\n",
    "    ## Inference ##\n",
    "    ###############\n",
    "\n",
    "    model.eval()  \n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "\n",
    "            val_matrix, val_cluster = val_batch\n",
    "            \n",
    "            val_outputs_, val_labels_ = model(val_matrix, val_cluster)\n",
    "            one_hot_labels_val_ = F.one_hot(val_labels_,2)\n",
    "            \n",
    "            \n",
    "            val_loss += criterion(val_outputs_, one_hot_labels_val_.float()).item()\n",
    "            \n",
    "            _, val_predicted = torch.max(val_outputs_.data, 1)\n",
    "            val_total_samples += one_hot_labels_val_.size(0)\n",
    "            val_correct_predictions += (val_predicted == val_labels_).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct_predictions / val_total_samples\n",
    "        val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {accuracy:.4f}, '\n",
    "          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        \n",
    "    current_time = datetime.datetime.now()\n",
    "    checkpoint_name = f\"{current_time.strftime('%m%d_%H%M%S')}_{epoch + 1}third_trial_lr=0.001.pt\"    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
